"""
Phase 4 Training - Continue from Checkpoint 1901
================================================
Continue fine-tuning with new 40 selected VÃ¡gÃ³ samples
Starting point: best_model_1901.pth (Mel CE: 2.971)
Target: Mel CE < 2.5 (excellent quality)

New dataset: 40 high-quality samples with distinct characteristics
- 10 excitement samples
- 14 neutral samples
- 16 question samples
"""

import os
from pathlib import Path
from trainer import Trainer, TrainerArgs
from TTS.config.shared_configs import BaseDatasetConfig
from TTS.tts.datasets import load_tts_samples
from TTS.tts.layers.xtts.trainer.gpt_trainer import GPTArgs, GPTTrainer, GPTTrainerConfig, XttsAudioConfig

# Configuration
OUTPUT_PATH = "run/training_phase4_continuation"
DATASET_PATH = "dataset_phase4"  # New 40 selected samples
RESUME_CHECKPOINT = "run/training_combined_phase2/XTTS_Combined_Phase2-October-04-2025_03+00PM-fb239cd/best_model_1901.pth"

# Training parameters - PHASE 4 (focused fine-tuning)
BATCH_SIZE = 2  # Smaller batch for focused learning
NUM_EPOCHS = 50  # Extended training for deeper learning
LEARNING_RATE = 5e-7  # Very low for fine refinement from 2.971
EVAL_SPLIT_SIZE = 0.20  # 20% for evaluation (8 samples)

print("=" * 80)
print("ğŸ¯ PHASE 4 TRAINING - CONTINUATION FROM CHECKPOINT 1901")
print("=" * 80)
print()
print("ğŸ“Š Current Status:")
print("   â€¢ Starting Mel CE: 2.971 (checkpoint 1901)")
print("   â€¢ Target Mel CE: < 2.5 (excellent quality)")
print("   â€¢ Improvement needed: -15.8%")
print()
print("ğŸ“¦ New Dataset:")
print("   â€¢ 40 high-quality selected samples")
print("   â€¢ 10 excitement + 14 neutral + 16 question")
print("   â€¢ Distinct characteristics for better prosody")
print()
print("ğŸ”§ Phase 4 Configuration:")
print(f"   â€¢ Resuming from: best_model_1901.pth")
print(f"   â€¢ Learning Rate: {LEARNING_RATE} (ultra-low)")
print(f"   â€¢ Batch Size: {BATCH_SIZE} (focused learning)")
print(f"   â€¢ Epochs: {NUM_EPOCHS}")
print(f"   â€¢ Focus: Mel CE improvement + prosody diversity")
print()
print("=" * 80)
print()

# Verify checkpoint exists
if not Path(RESUME_CHECKPOINT).exists():
    print(f"âŒ ERROR: Checkpoint not found!")
    print(f"   Looking for: {RESUME_CHECKPOINT}")
    exit(1)

print(f"âœ… Found checkpoint: {Path(RESUME_CHECKPOINT).name}")
print()

# Verify dataset exists
if not Path(DATASET_PATH).exists():
    print(f"âŒ ERROR: Dataset not found!")
    print(f"   Looking for: {DATASET_PATH}")
    print()
    print("ğŸ’¡ Run this first:")
    print("   python scripts/prepare_phase4_dataset.py")
    exit(1)

print(f"âœ… Found dataset: {DATASET_PATH}")
print()

# RUN_NAME for this session
RUN_NAME = "XTTS_Phase4_Continuation"

# DVAE checkpoint
DVAE_CHECKPOINT_LINK = "https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/dvae.pth"
MEL_NORM_LINK = "https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/mel_stats.pth"

# Download manager
CHECKPOINTS_OUT_PATH = Path("models/")
CHECKPOINTS_OUT_PATH.mkdir(parents=True, exist_ok=True)

def download_if_needed(url, filename):
    filepath = CHECKPOINTS_OUT_PATH / filename
    if not filepath.exists():
        print(f"   Downloading {filename}...")
        import requests
        response = requests.get(url)
        filepath.write_bytes(response.content)
        print(f"   âœ… Downloaded: {filename}")
    else:
        print(f"   âœ… Found: {filename}")
    return str(filepath)

# Download if needed
print("ğŸ“¥ Checking DVAE checkpoint...")

DVAE_CHECKPOINT = download_if_needed(DVAE_CHECKPOINT_LINK, "dvae.pth")
MEL_NORM_FILE = download_if_needed(MEL_NORM_LINK, "mel_stats.pth")

print()

# Dataset configuration
print("ğŸ“‚ Loading dataset...")
config_dataset = BaseDatasetConfig(
    formatter="coqui",
    dataset_name="phase4_selected_vago",
    path=DATASET_PATH,
    meta_file_train="metadata.csv",
    language="hu",
)

# Model paths from original training
model_path = "C:\\Users\\szolzol\\AppData\\Local\\tts\\tts_models--multilingual--multi-dataset--xtts_v2"

# Audio config
audio_config = XttsAudioConfig(sample_rate=22050, dvae_sample_rate=22050, output_sample_rate=24000)

# XTTS GPT configuration - Phase 4 ultra-low LR
print("ğŸ”§ Configuring XTTS GPT Trainer...")
model_args = GPTArgs(
    max_conditioning_length=143677,  # 6 secs
    min_conditioning_length=66150,   # 3 secs
    debug_loading_failures=False,
    max_wav_length=255995,  # ~11 seconds
    max_text_length=300,
    mel_norm_file=MEL_NORM_FILE,
    dvae_checkpoint=DVAE_CHECKPOINT,
    xtts_checkpoint=None,  # Will be loaded from resume
    tokenizer_file=f"{model_path}\\vocab.json",
    gpt_num_audio_tokens=1026,
    gpt_start_audio_token=1024,
    gpt_stop_audio_token=1025,
    gpt_use_masking_gt_prompt_approach=True,
    gpt_use_perceiver_resampler=True,
)

config = GPTTrainerConfig(
    epochs=NUM_EPOCHS,
    output_path=OUTPUT_PATH,
    model_args=model_args,
    run_name=RUN_NAME,
    project_name="XTTS_trainer",
    run_description="Phase 4 - Continue from 1901 with selected samples for Mel CE < 2.5",
    dashboard_logger="tensorboard",
    logger_uri=None,
    audio=audio_config,
    batch_size=BATCH_SIZE,
    batch_group_size=0,
    eval_batch_size=BATCH_SIZE,
    num_loader_workers=0,
    eval_split_max_size=256,
    eval_split_size=EVAL_SPLIT_SIZE,
    print_step=25,  # More frequent prints for small dataset
    plot_step=50,
    log_model_step=50,
    save_step=50,  # Save more frequently
    save_n_checkpoints=3,  # Keep last 3 checkpoints
    save_checkpoints=True,
    save_all_best=True,
    save_best_after=50,
    target_loss="loss",
    print_eval=True,
    run_eval=True,
    run_eval_steps=50,  # Evaluate frequently
    test_sentences=[],
    
    # Phase 4: Ultra-low learning rate for gentle refinement
    lr=LEARNING_RATE,
    optimizer="AdamW",
    optimizer_params={"betas": [0.9, 0.96], "eps": 1e-8, "weight_decay": 1e-2},
    lr_scheduler="MultiStepLR",
    lr_scheduler_params={"milestones": [50000 * 18, 150000 * 18], "gamma": 0.5},
)

# Load samples
print("ğŸ“Š Loading samples...")
train_samples, eval_samples = load_tts_samples(
    config_dataset,
    eval_split=True,
    eval_split_max_size=config.eval_split_max_size,
    eval_split_size=config.eval_split_size,
)

print(f"   Training samples: {len(train_samples)}")
print(f"   Evaluation samples: {len(eval_samples)}")
print()

# Validate dataset size
if len(train_samples) < 20:
    print("âš ï¸  WARNING: Very small training set!")
    print("   Consider adding more samples for better results")
    print()

# Initialize model from config
print("ğŸ¤– Initializing model...")
model = GPTTrainer.init_from_config(config)
print("   âœ… Model initialized")
print()

# Trainer - will restore from checkpoint
print("ğŸ“ Setting up trainer (will restore from checkpoint 1901)...")
trainer = Trainer(
    TrainerArgs(
        restore_path=RESUME_CHECKPOINT,  # Resume from best_model_1901
        skip_train_epoch=False,
        start_with_eval=True,
        grad_accum_steps=1,
    ),
    config,
    output_path=OUTPUT_PATH,
    model=model,
    train_samples=train_samples,
    eval_samples=eval_samples,
)

print("   âœ… Checkpoint 1901 loaded successfully")
print()

# Add automatic checkpoint cleanup
print("ğŸ§¹ Setting up automatic checkpoint cleanup...")
original_save_checkpoint = trainer.save_checkpoint

def save_checkpoint_with_cleanup():
    """Save checkpoint and cleanup old ones"""
    original_save_checkpoint()
    
    from pathlib import Path
    output_folder = Path(OUTPUT_PATH) / RUN_NAME
    if output_folder.exists():
        # Keep only last 3 checkpoints
        checkpoints = sorted(output_folder.glob("checkpoint_*.pth"))
        if len(checkpoints) > 3:
            for old_checkpoint in checkpoints[:-3]:
                try:
                    size_mb = old_checkpoint.stat().st_size / (1024**2)
                    old_checkpoint.unlink()
                    print(f"  ğŸ—‘ï¸  Deleted: {old_checkpoint.name} ({size_mb:.0f} MB freed)")
                except Exception as e:
                    print(f"  âš ï¸  Could not delete {old_checkpoint.name}: {e}")

trainer.save_checkpoint = save_checkpoint_with_cleanup
print("   âœ… Auto-cleanup enabled (keeps last 3 checkpoints)")
print()

print()
print("=" * 80)
print("ğŸš€ STARTING PHASE 4 TRAINING")
print("=" * 80)
print()
print("ğŸ“ˆ Training Strategy:")
print("   â€¢ Small dataset (40 samples) = focused learning")
print("   â€¢ Ultra-low LR (5e-7) = gentle refinement")
print("   â€¢ Extended epochs (50) = deep optimization")
print("   â€¢ Starting from best_model_1901 (Mel CE: 2.971)")
print()
print("ğŸ¯ Goals:")
print("   1. Mel CE < 2.5 (excellent quality)")
print("   2. Maintain Text CE < 0.03")
print("   3. Improve prosody diversity (3 distinct categories)")
print("   4. Preserve VÃ¡gÃ³ voice characteristics")
print()
print("â±ï¸  Estimated time: 2-4 hours")
print("ğŸ’¾ Best models saved automatically")
print("ğŸ“Š Check TensorBoard for real-time metrics")
print()
print("ğŸ” Key Metrics to Watch:")
print("   â€¢ Mel CE (target < 2.5)")
print("   â€¢ Text CE (should stay < 0.03)")
print("   â€¢ Training Loss (should decrease steadily)")
print("   â€¢ Eval Loss (should follow training loss)")
print()
print("=" * 80)
print()

# Train!
trainer.fit()

print()
print("=" * 80)
print("âœ… PHASE 4 TRAINING COMPLETE!")
print("=" * 80)
print()
print("ğŸ“Š Results Location:")
print(f"   Output folder: {OUTPUT_PATH}/{RUN_NAME}")
print(f"   Best model: {OUTPUT_PATH}/{RUN_NAME}/best_model_*.pth")
print(f"   Checkpoints: {OUTPUT_PATH}/{RUN_NAME}/checkpoint_*.pth")
print()
print("ğŸ¯ Next Steps:")
print("   1. Check final Mel CE score in logs")
print("   2. Update generate_questions_and_answers.py with new model path")
print("   3. Generate test samples to verify quality")
print("   4. Compare with checkpoint 1901 samples")
print()
print("ğŸ’¡ If Mel CE < 2.5:")
print("   âœ… Mission accomplished! Use this model for production")
print()
print("ğŸ’¡ If Mel CE still > 2.5:")
print("   â€¢ Continue training for more epochs")
print("   â€¢ Add more diverse samples to dataset")
print("   â€¢ Consider adjusting learning rate")
print()
