"""
Load trained XTTS model and generate quiz show samples
Handles trainer checkpoint format properly
"""

import os
import sys
import torch
import torchaudio
from pathlib import Path

# Add TTS to path
try:
    from TTS.tts.configs.xtts_config import XttsConfig
    from TTS.tts.models.xtts import Xtts
except ImportError as e:
    print(f"âŒ Error importing TTS: {e}")
    print("Please install: pip install TTS==0.22.0")
    sys.exit(1)

# Paths - PHASE 2 BEST MODEL
PROJECT_ROOT = Path("f:/CODE/tts-2")
MODEL_DIR = PROJECT_ROOT / "run" / "training_combined_phase2" / "XTTS_Combined_Phase2-October-04-2025_03+00PM-fb239cd"
OUTPUT_DIR = PROJECT_ROOT / "quiz_samples_phase2_final"

# Reference audio (use neutral reference from dataset)
REFERENCE_AUDIO = "dataset_combined/neutral/neutral_002.wav"

# Quiz show phrases - 5 complete questions with A/B/C/D options (with longer pauses between answers)
TEST_PHRASES = [
    ("q1_geography", "Melyik orszÃ¡g fÅ‘vÃ¡rosa Budapest? MagyarorszÃ¡g... RomÃ¡nia... Ausztria... vagy SzlovÃ¡kia."),
    ("q2_history", "Melyik Ã©vben fedezte fel Kolumbusz AmerikÃ¡t? EzernÃ©gyszÃ¡zkilencvenkettÅ‘... ezernÃ©gyszÃ¡znyolcvannyolc... ezernÃ©gyszÃ¡zhetvenhat... vagy ezernÃ©gyszÃ¡zkilencvenhat."),
    ("q3_science", "HÃ¡ny proton van egy hidrogÃ©n atom magjÃ¡ban? Egy proton... kettÅ‘ proton... hÃ¡rom proton... vagy nÃ©gy proton."),
    ("q4_literature", "Ki Ã­rta a RÃ³meÃ³ Ã©s JÃºliÃ¡t? William Shakespeare... Charles Dickens... Victor Hugo... vagy Mark Twain."),
    ("q5_sports", "HÃ¡ny jÃ¡tÃ©kos jÃ¡tszik egy futballcsapatban egyszerre? Kilenc jÃ¡tÃ©kos... tÃ­z jÃ¡tÃ©kos... tizenegy jÃ¡tÃ©kos... vagy tizenkettÅ‘ jÃ¡tÃ©kos."),
]


def load_trained_model():
    """Load the trained model from trainer checkpoint"""
    print("="*60)
    print("LOADING TRAINED MODEL")
    print("="*60)
    
    config_path = MODEL_DIR / "config.json"
    checkpoint_path = MODEL_DIR / "best_model_1901.pth"  # Phase 2 best model (Mel CE: 2.971)
    
    if not config_path.exists():
        print(f"âŒ Config not found: {config_path}")
        return None
    
    if not checkpoint_path.exists():
        print(f"âŒ Checkpoint not found: {checkpoint_path}")
        return None
    
    print(f"âœ“ Config: {config_path.name}")
    print(f"âœ“ Checkpoint: {checkpoint_path.name}")
    print(f"âœ“ Size: {checkpoint_path.stat().st_size / 1024**3:.2f} GB")
    
    try:
        # Load config
        print("\nğŸ“‹ Loading configuration...")
        config = XttsConfig()
        config.load_json(str(config_path))
        
        # Initialize model  
        print("ğŸ¤– Initializing model...")
        model = Xtts.init_from_config(config)
        
        # Load checkpoint using model's built-in method
        # This properly initializes tokenizer and other components
        print("ğŸ’¾ Loading checkpoint...")
        vocab_path = MODEL_DIR / "vocab.json"
        if not vocab_path.exists():
            print(f"âŒ Vocab file not found: {vocab_path}")
            return None
        
        model.load_checkpoint(
            config,
            checkpoint_dir=str(MODEL_DIR),
            checkpoint_path=str(checkpoint_path),
            vocab_path=str(vocab_path),  # CRITICAL: needed for tokenizer
            eval=True,
            use_deepspeed=False
        )
        print("âœ“ Checkpoint loaded with tokenizer initialized")
        
        # Move to GPU if available
        if torch.cuda.is_available():
            print("ğŸ® Moving model to GPU...")
            model.cuda()
            print("âœ“ Model on GPU")
        else:
            print("ğŸ’» Using CPU (will be slower)")
        
        model.eval()
        print("âœ“ Model loaded successfully!")
        
        return model, config
        
    except Exception as e:
        print(f"âŒ Error loading model: {e}")
        import traceback
        traceback.print_exc()
        return None


def generate_samples(model, config):
    """Generate test samples for all categories"""
    print("\n" + "="*60)
    print("GENERATING TEST SAMPLES")
    print("="*60)
    
    OUTPUT_DIR.mkdir(exist_ok=True)
    
    total_samples = len(TEST_PHRASES)
    success_count = 0
    
    for i, (category, text) in enumerate(TEST_PHRASES, 1):
        output_file = OUTPUT_DIR / f"{i:02d}_{category}.wav"
        ref_audio = REFERENCE_AUDIO  # Use single high-quality reference
        
        print(f"\n[{i}/{total_samples}] {category.upper()}")
        print(f"Text: {text}")
        print(f"Ref: {Path(ref_audio).name}")
        
        try:
            # Get conditioning latents from reference audio
            gpt_cond_latent, speaker_embedding = model.get_conditioning_latents(
                audio_path=[ref_audio]
            )
            
            # Extremely low temperature for maximum stability
            # 0.40 = ultra stable, perfectly even delivery
            temperature = 0.40
            
            char_count = len(text)
            print(f"Length: {char_count} chars â†’ temp={temperature}")
            
            # Generate speech using inference method
            out = model.inference(
                text=text,
                language="hu",
                gpt_cond_latent=gpt_cond_latent,
                speaker_embedding=speaker_embedding,
                temperature=temperature,  # Ultra low temp for maximum stability
                top_p=0.80,  # Lower for more predictability
                top_k=40,    # Lower for less variation
                repetition_penalty=6.0,  # Higher to prevent emphasis spikes
                length_penalty=1.0,
            )
            
            # Save audio
            torchaudio.save(
                str(output_file),
                torch.tensor(out["wav"]).unsqueeze(0),
                24000
            )
            
            print(f"âœ… Saved: {output_file.name}")
            success_count += 1
            
        except Exception as e:
            print(f"âŒ Error: {e}")
            import traceback
            traceback.print_exc()
    
    print("\n" + "="*60)
    print("GENERATION COMPLETE")
    print("="*60)
    print(f"âœ… Generated: {success_count}/{total_samples} samples")
    print(f"ğŸ“ Output: {OUTPUT_DIR}")
    
    if success_count > 0:
        print("\nğŸ§ Listen to the samples!")
        print(f"   Generated: {OUTPUT_DIR}")
        print(f"\nğŸ¯ Phase 2 Model: best_model_1901.pth (Mel CE: 2.971, -41.1% improvement)")
        print("\nğŸ“Š Compare:")
        print("   - Voice similarity to IstvÃ¡n VÃ¡gÃ³")
        print("   - Quiz show energy and enthusiasm")
        print("   - Natural Hungarian pronunciation")
        print("   - Smoothness (no choppiness)")


def main():
    print("="*60)
    print("XTTS-V2 TRAINED MODEL TESTING")
    print("IstvÃ¡n VÃ¡gÃ³ - Milliomos Quiz Show Voice")
    print("="*60)
    
    # Load model
    result = load_trained_model()
    if result is None:
        print("\nâŒ Failed to load model")
        sys.exit(1)
    
    model, config = result
    
    # Generate samples
    print("\nâ³ Starting generation (this may take a few minutes)...")
    generate_samples(model, config)
    
    print("\nâœ… All done!")


if __name__ == "__main__":
    main()
